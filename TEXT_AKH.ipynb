{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TEXT_AKH.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KampHost/caldiss/blob/master/TEXT_AKH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57BxQnSVNnyZ",
        "colab_type": "text"
      },
      "source": [
        "#Portfolio Text Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Glw8wcPNOE4_",
        "colab_type": "text"
      },
      "source": [
        "### Assignment 1: \n",
        "1. Take the following text and transform it into a list of lists with with each element being a tokenized sentence.\n",
        "2. Remove stopwords, lower all tokens and keep only alpha-numeric tokens.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vfnn8n7IOezF",
        "colab_type": "text"
      },
      "source": [
        "### Preamble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl9wU4HJOcOY",
        "colab_type": "code",
        "outputId": "54dd8500-2192-4e68-a500-3522b8b5eb50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', -1) #to see more text\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "from collections import Counter\n",
        "# very short RegEx\n",
        "import re\n",
        "import nltk #this part is needed on colab.\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "# Tokenizing sentences\n",
        "from nltk.tokenize import sent_tokenize\n",
        "# Tokenizing words\n",
        "from nltk.tokenize import word_tokenize\n",
        "# Tokenizing Tweets made easy!\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8tLpOYZMRMU",
        "colab_type": "text"
      },
      "source": [
        "## Data and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ggxkaVnNpKD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"Iâ€™ve been called many things in my life, but never an optimist. That was fine by me. I believed pessimists lived in a constant state of pleasant surprise: if you always expected the worst, things generally turned out better than you imagined. The only real problem with pessimism, I figured, was that too much of it could accidentally turn you into an optimist.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jHfpFD0Oq36",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split on fullstop\n",
        "text.lower().split(\".\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lz0_4RiiOvEw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split on empty space\n",
        "text.split(\" \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ulcugzm7PIvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining stopwords\n",
        "stopwords_en = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', \n",
        "                'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \n",
        "                \"you'd\", 'your', 'yours', 'yourself', 'yourselves', \n",
        "                'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', \n",
        "                'hers', 'herself', 'it', \"it's\", 'its', 'itself', \n",
        "                'they', 'them', 'their', 'theirs', 'themselves', 'what', \n",
        "                'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', \n",
        "                'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', \n",
        "                'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', \n",
        "                'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', \n",
        "                'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', \n",
        "                'between', 'into', 'through', 'during', 'before', 'after', 'above', \n",
        "                'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', \n",
        "                'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', \n",
        "                'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', \n",
        "                'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', \n",
        "                'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', \n",
        "                'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', \n",
        "                'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \n",
        "                \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \n",
        "                \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", \n",
        "                'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \n",
        "                'won', \"won't\", 'wouldn', \"wouldn't\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEL934lrPLH7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Keeping only words that are not stopwords\n",
        "[word for word in text.lower().split() if word not in stopwords_en]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbJzVsFSPSjK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop fullstop and numbers\n",
        "'.,'.strip(r'[\" ,.!?:;\"]')\n",
        "[word.strip(r'[\" ,.!?:;\"]') for word in text.lower().split() if word not in stopwords_en and not word.isdigit()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZ89kxq_Pa9I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Getting the stences\n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vI3jZERlP0IV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use word_tokenize to tokenize the third sentence: tokenized_sent\n",
        "tokenized_sent = word_tokenize(sentences[1])\n",
        "# Make a set of unique tokens in the entire scene: unique_tokens\n",
        "unique_tokens = set(word_tokenize(text))\n",
        "print(tokenized_sent)\n",
        "print(unique_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXEOnRa7P2n_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBDKHKnjQMDn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[word.lower() for word in word_tokenize(text) if word not in stop_words and word.isalnum()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MEWN3_aTHMU",
        "colab_type": "text"
      },
      "source": [
        "### Assignment 2: \n",
        "1. Make a list of the most common hashtags.\n",
        "Get the hashtags from the text, not using the column containing them already"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHBvEm_8CD1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', -1) #to see more text\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "import itertools\n",
        "from collections import Counter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDLX4FI3CH7J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenizing Tweets made easy!\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "tknzr = TweetTokenizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DuyBkFhTlxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets_df = pd.read_json('https://github.com/CALDISS-AAU/sdsphd19_coursematerials/raw/master/data/tweets_boomer.zip')\n",
        "tweets_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C918UjIuOoPG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reset index (not really needed but why not)\n",
        "tweets_df = tweets_df.set_index(pd.to_datetime(tweets_df.created_at))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOJfnI-COzGM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Identifying hashtags\n",
        "tweets_df['tags'] = tweets_df['tweet'].map(lambda textline: [tag for tag in tknzr.tokenize(textline) if tag.startswith('#')])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A0lfjqzP7aX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Only keep tweets where a hashtag i present\n",
        "tweets_df = tweets_df[tweets_df['tags'].map(len) > 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCiyi6pfCY9M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Collect\n",
        "tags = itertools.chain(*tweets_df['tags'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTUXzO2GCe8l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "counted_tags = Counter(tags)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frXYHMCaCrig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "counted_tags.most_common()[:11]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGFvyyu0J2Gt",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 3\n",
        "\n",
        "###Perform an LDA analysis of the #OKBoomer dataset\n",
        "\n",
        "- Filter the corpus using tweet-preprocessor - try to figure out how to use it using it's documentation\n",
        "- Clean up further with SpaCy (keep only ADV, ADJ, NOUN)\n",
        "- Use Gensim to build a Dictionary (Filter extremes) and Corpus\n",
        "- Use Gensim to run LDA\n",
        "- Identify 10 topics\n",
        "- Plot topic-counts by day"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHnzxWMeKgbp",
        "colab_type": "text"
      },
      "source": [
        "###Preamble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaU1vQqiK76B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load data\n",
        "boomer = pd.read_json('https://github.com/CALDISS-AAU/sdsphd19_coursematerials/raw/master/data/tweets_boomer.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlHUOPa_wmma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Filter the corpus using tweet-preprocessor\n",
        "! pip install tweet-preprocessor\n",
        "import preprocessor as p\n",
        "p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.HASHTAG)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RfzO8tCmbK1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_tweet= boomer['tweet'].map(p.clean)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNiSZeiCmfBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_tweet=pd.DataFrame(clean_tweet)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bYnEL0UopJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SPACY tokenizer and filter\n",
        "import spacy\n",
        "nlp = spacy.load(\"en\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ze6JUzo9LOiO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokens = []\n",
        "for tweet in nlp.pipe(clean_tweet['tweet']):\n",
        "  proj_tok = [token.lemma_.lower() for token in tweet if token.pos_ in ['NOUN', 'ADJ', 'ADV'] and not token.is_stop] \n",
        "  tokens.append(proj_tok)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8_csK2CByG2",
        "colab_type": "code",
        "outputId": "efbdef89-f528-4fe2-e06a-98a75a8ba39c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Create a Dictionary \n",
        "!pip install -qq -U gensim\n",
        "from gensim.corpora.dictionary import Dictionary"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24.2MB 100kB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQMj9OcR0PcF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a Dictionary from the tokenized tweets\n",
        "dictionary = Dictionary(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWhpbFl0CRDt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# filter out low-/high-frequency stuff, also limit the vocabulary to max 1000 words\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFygTJxlCtL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# construct corpus using this dictionary\n",
        "corpus = [dictionary.doc2bow(doc) for doc in tokens]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ddm8JJ4tC46C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we'll use the faster multicore version of LDA\n",
        "from gensim.models import LdaMulticore"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1KsMUK6C9gy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training the model (makes some mess atm due to version clashes)\n",
        "lda_model = LdaMulticore(corpus, id2word=dictionary,  num_topics=10, workers = 4, passes=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5XLn4Pl-AS2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Based on the model predict topic for each tweet\n",
        "predict_topic=[]\n",
        "for result in lda_model[corpus]:\n",
        "  prediction=sorted(result, key=lambda x: -x[1])[0][0]\n",
        "  predict_topic.append(prediction)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6v65bxj___-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "boomer['predicted'] = predict_topic"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3r2_DdkKA3-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "topic_list = list(set(predict_topic))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHhVq26oAu2k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for col in topic_list:\n",
        "  boomer[col] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axhNxNP_BbmG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for topic in topic_list:\n",
        "  boomer[topic] = boomer['predicted'].map(lambda t: 1 if topic==t else 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ip3wuAlbCSSP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ser Index to datetime\n",
        "boomer.set_index(pd.to_datetime(boomer.created_at), inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-a2x7NBjcGR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "boomer.resample('D')[predict_topic].sum().plot()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}